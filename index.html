<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Benchmarking Vision Language Models for Cultural Understanding"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Benchmarking Vision Language Models for Cultural Understanding</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Benchmarking Vision Language Models for Cultural Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://bajuka.github.io" target="_blank">Shravan Nayak</a><sup>1,2</sup></span> &nbsp;
                <span class="author-block">
                  <a href="https://kanji95.github.io" target="_blank">Kanishk Jain</a><sup>1,2</sup></span> &nbsp;
                  <span class="author-block">
                    <a href="https://rabiul.me" target="_blank">Rabiul Awal</a><sup>1</sup>
                  </span>
                  <br>
                  <span class="author-block">
                    <a href="https://sivareddy.in/" target="_blank">Siva Reddy</a><sup>1,3</sup> &nbsp;
                  </span>
                  <span class="author-block">
                    <a href="https://www.sjoerdvansteenkiste.com/" target="_blank">Sjoerd van Steenkiste</a><sup>4</sup> &nbsp;
                  </span>
                  <span class="author-block">
                    <a href="https://lisaanne.github.io/" target="_blank">Lisa Anne Hendricks</a><sup>4</sup>
                  </span>
                  <br>
                  <span class="author-block">
                  </span>
                    <a href="https://kstanczak.github.io/" target="_blank">Karolina Stanczak</a><sup>1,3</sup> &nbsp;
                  </span>
                  <span class="author-block">
                    <a href="https://www.iro.umontreal.ca/~agrawal/" target="_blank">Aishwarya Agrawal</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Mila - Quebec AI Institute<sup>1</sup> &emsp; Université de Montréal<sup>2</sup>
                      <br>McGill University<sup>3</sup> &emsp; Google DeepMind<sup>4</sup></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-database"></i>
                        </span>
                        <span>Dataset (incoming)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (incoming)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.10920" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/culture_vqa_samples.jpg" id="tree" alt="cultural vqa dataset random samples" style="width:100%; height:auto;"> 
      <br><br>
      <p class="subtitle has-text-centered">
        Samples from CulturalVQA. Our dataset is comprised of images presenting cultural concepts from 11 countries across five facets: traditions, rituals, food, drink, and clothing. It further includes questions probing cultural understanding of the concepts presented in the images and answers to these questions.
      </p>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension.
          This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a diverse collection of 2378 image - question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions.
 Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink.   
  These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->


<!-- Dataset Analysis-->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Dataset analysis</h2>
      <div class = "content has-text-justified">
      <p><strong>Images, Question and Answer:</strong> Our dataset comprises of 2328 unique images. We collected 2378 questions in total from 11 countries asking the local annotators to create questions for images from their culture such that it is easy for the locals to answer but challenging for outsiders. We present the number of unique questions per country (below). Our dataset consists of 7206 manually curated answers in total (each question has 1-5 answers).</p>    
      <figure class="image is-inline-block">
        <img src="static/images/comprehensive_country_data_analysis.png" alt="anlaysis">
      </figure>
      <p style="text-align: center;">Comparative analysis of data by country. The figure presents three aspects: (A) unique counts of images, questions, and answers, (B) average lengths of questions and answers, and (C) average number of answers and inter-annotator agreement scores across countries, showcasing variations and trends in CulturalVQA.</p>
      </div>

      <div class="content has-text-justified">
      <p><strong>Cultural concepts:</strong> According to the pie chart in the figure below, food-related questions are most prevalent, accounting for 31.6% of the dataset, followed closely by traditions and rituals, which represent 28.6% and 22.6% respectively. Thus, roughly 50% of the questions in our dataset probe for cultural understanding of the intangible aspects of culture (rituals and traditions)! The <strong>word clouds</strong> generated from the collected answers reveal diverse expressions of rituals and traditions represented by terms like <em>hamam</em> (Turkey) and <em>meskel</em> (Ethiopia). Further, the food category includes diverse items such as <em>feijoada</em> (Brazil), <em>fufu</em> (Nigeria), and <em>vada</em> (India) indicating a geo-diverse culinary scope. While the clothing category is the least prevalent in the dataset, it shows the highest variety in terms of collected answers.</p>      
      <figure class="image is-inline-block">
        <img src="static/images/facets.png" alt="anlaysis">
      </figure>
      <p style="text-align: center;">Word clouds representing the answers in CulturalVQA across five facets of culture: clothing, drink, food, rituals, and traditions. In the bottom right, a breakdown of cultural facets in data is depicted.</p>
      </div> 
  </div>
</section>
<!-- End of Dataset Analysis-->
<!-- Model Performance-->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Benchmarking</h2>

    <h3 class="title is-4">Degree of visual understanding required</h3>
    <p>Our evaluations with GPT-4 show that while adding country and, visual contexts in the form of google lens entities improves accuracy, they still fall short compared to the full vision-language model (VLM) that integrates detailed visual information. This confirms that accurate answers require a substantial degree of visual comprehension.</p>      
    <div class="content has-text-justified">
      <figure>
        <img style="height: 500px; width: 500px; display: block; margin: auto;" src="static/images/llm_only_performance.png" alt="model performance">
      </figure>
      <p style="text-align: center;">
        Baseline evaluation of the degree of visual understanding required in CulturalVQA: LLM-only, LLM with a country-specific context, LLM with Google Lens entities, and GPT-4V.
      </p>
    </div>

    <h3 class="title is-4">Open-source and closed-source models</h3>
    <div class="content has-text-justified">
      <p>We report the LAVE scores of open-source and closed-source vision-language models on the proposed CulturalVQA benchmark, which range across countries from 43% to 72% for GPT-4, the best-performing model. Notably, there's a considerable performance gap between closed-source models and the best open-source models. This gap is striking in African-Islamic cultures (Ethiopia, Nigeria, Iran, and Turkey), with a 29.7% gap for Ethiopia, where models struggle the most.</p>
      <figure class="image is-inline-block">
        <img src="static/images/model_performance.png" alt="model performance">
      </figure>
      <p style="text-align: center;">
        LAVE accuracies of open- and closed-source models on CulturalVQA. Best-performing results per country are highlighted in <span style="background-color: rgb(146, 241, 146)">green</span>, and best-performing results among open-source models are highlighted in <span style="background-color: rgb(148, 181, 233)">blue</span>.
      </p>
    </div>

    <h3 class="title is-4">Performance across facets</h3>
    <div class="content has-text-justified">
      <p>We report the model performance across 5 cultural facets: food, drinks, clothing, rituals and traditions. Generally, we find that proprietary models tend to perform better on intangible concepts - rituals, and traditions, compared to drink and food. Indeed, the highest performance of GPT-4 is achieved in the rituals facet (>60%), whereas in the clothing facet, it achieves a lower performance of ~ 53%</p>
      <figure>
        <img style="height: 400px; width: 600px; display: block; margin: auto;" src="static/images/facet_wise_accuracy.png" alt="model performance">
      </figure>
      <p style="text-align: center;">
        VLM performance across facets as measured using LAVE accuracies.
      </p>
    </div>

  <h3 class="title is-4">Human vs. AI Performance on CulturalVQA</h3>
    <div>
      <p>In our evaluation using the LAVE metric on 1,455 questions, humans familiar with the specific cultures achieved accuracy between 55% and 85%, with countries like Iran seeing scores over 80%. However, a significant gap was observed when comparing these results to AI performance using GPT-4, particularly in non-Western countries such as  Iran, Nigeria, India, Turkey, and Ethiopia, where the discrepancy exceeded 10%. This highlights the AI's stronger grasp of Western cultural concepts due to their more prevalent representation in its training data.</p>
      <br>
      <figure>
        <img style="height: 400px; width: 600px; display: block; margin: auto;" src="static/images/performance_difference.png" alt="model performance">
      </figure>
      <p style="text-align: center;">
        Performance gap between the best open-source and closed-source models compared to human performance. Negative values indicate where models underperform relative to humans.
      </p>
    </div>

  <br>
  <h3 class="title is-4">Qualitative Analysis of GPT-4</h3>
    <div>
      <figure>
        <img src="static/images/qualitative.png" alt="model performance">
      </figure>
      <p style="text-align: center;">
        Qualitative failure examples of GPT-4 predictions.
      </p>
      <br>
      <p>Our qualitative evaluation of the best-performing model, GPT-4, highlights its limitations in recognizing and interpreting cultural nuances. For instance, GPT-4 overlooks the cultural significance of intangible cultural concepts like coral beads in Nigeria, which symbolize wealth and heritage but are treated merely as decorative objects, as well as it fails to recognize the symbolic connection between cows and planet Earth in Indian culture (see figure above).
      Focusing on tangible cultural concepts, the model's shortcomings are evident as it inaccurately recognizes cultural entities and objects. For instance, it mislabels <i>Naghali</i>, a traditional Iranian storyteller as a Dervish and mistakes a traditional Turkish tea glass for a tulip glass, commonly used for serving beer. These examples reveal how GPT-4 struggles with both tangible and intangible cultural concepts: it has difficulties distinguishing between visually similar but culturally distinct entities and objects, and it lacks a deep understanding of cultural beliefs and symbolic meanings.</p>
    </div>
  </div>

  </section>
<!-- End of Model Performance-->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <p>If you found this work useful in your own research, please consider citing the following:</p>
      <pre><code>
        @misc{nayak2024benchmarkingvisionlanguagemodels,
          title={Benchmarking Vision Language Models for Cultural Understanding}, 
          author={Shravan Nayak and Kanishk Jain and Rabiul Awal and Siva Reddy and Sjoerd van Steenkiste and Lisa Anne Hendricks and Karolina Stańczak and Aishwarya Agrawal},
          year={2024},
          eprint={2407.10920},
          archivePrefix={arXiv},
          primaryClass={cs.CV},
          url={https://arxiv.org/abs/2407.10920}, 
    }
      </code></pre>
  </div>
</section>

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
